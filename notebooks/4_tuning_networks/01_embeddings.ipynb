{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import tokenizers as tk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizers\n",
    "Because we can want to work with a vectorspace in the hidden layers of our model, the question is, how do we convert words into vectors? The answer is tokenization. \n",
    "With tokenization we map strings to arbitrary integers, and use the integers to look up a vector in a table (the embedding).\n",
    "\n",
    "In mathematical terms:\n",
    "\n",
    "$$\n",
    "f\\colon \\text{str} \\rightarrow \\mathbb{N}\\\\ \n",
    "g\\colon \\mathbb{N} \\rightarrow \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "where $f$ is the tokenizer and $g$ is the embedding function.\n",
    "\n",
    "### What is BPE?\n",
    "\n",
    "BPE (Byte Pair Encoding) is a subword tokenization algorithm. Instead of splitting words into individual characters or using entire words as tokens, BPE breaks words into smaller subword units. It starts with individual characters and merges the most frequent pairs of characters iteratively, creating subwords that can effectively represent both common words and rare or new words through combinations of these subwords.\n",
    "A rough outline of the BPE algorithm is as follows:\n",
    "1.\tStart with characters: Initially, BPE represents each word as a sequence of characters.\n",
    "2.\tIterative merging: It then identifies the most frequent pairs of characters and merges them into a single token. This process continues, merging pairs iteratively until the specified vocabulary size is reached.\n",
    "3.\tHandling rare words: With BPE, rare words that haven’t been seen during training can still be decomposed into recognizable subword tokens (e.g., “unhappiness” might become [“un”, “happy”, “ness”]).\n",
    "\n",
    "### Why is BPE better than a naive tokenizer?\n",
    "\n",
    "A naive tokenizer splits on spaces, treating each word as a token. This approach doesn’t work well for rare words, misspellings, or words not seen during training, as they would each be treated as unique tokens. BPE helps by breaking these words into smaller subword units, ensuring that even rare or new words can still be tokenized into meaningful subparts, reducing the size of the vocabulary and improving generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildBPE(corpus: list[str], vocab_size: int) -> tk.Tokenizer:\n",
    "    tokenizer = tk.Tokenizer(tk.models.BPE())\n",
    "    trainer = tk.trainers.BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        min_frequency=1,\n",
    "        special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<mask>\"],\n",
    "    )\n",
    "\n",
    "    # handle spaces better by removing the prefix space\n",
    "    tokenizer.pre_tokenizer = tk.pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "    tokenizer.decoder = tk.decoders.ByteLevel()\n",
    "\n",
    "    # train the BPE model\n",
    "    tokenizer.train_from_iterator(corpus, trainer)\n",
    "    # Padding is enabled to make sure input sequences match in length during training or inference.\n",
    "    tokenizer.enable_padding(pad_id=0, pad_token=\"<pad>\")\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special tokens:\n",
    "\n",
    "- Padding (`<pad>`): Padding ensures all input sequences are of equal length by adding this token where needed.\n",
    "- Start (`<s>`) and stop (`</s>`) tokens: These mark the beginning and end of a sequence, helping models understand where a sentence starts and ends.\n",
    "- Unknown (`<unk>`): This token is used for words or subwords that the tokenizer doesn’t know or hasn’t been trained on.\n",
    "- Mask (`<mask>`): This token is used in tasks like masked language modeling, where certain tokens are hidden, and the model is asked to predict them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We start with a simple corpus of two sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"the cat sat on the mat\", \"where is the cat\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = buildBPE(corpus, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, lets see how our vocabulary looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 3, 'cat': 2, 'sat': 1, 'on': 1, 'mat': 1, 'where': 1, 'is': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"First 10 tokens: {list(tokenizer.get_vocab())[:10]}\")\n",
    "print(f\"Last 10 tokens: {list(tokenizer.get_vocab())[-10:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now encode a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('the', 3),\n",
       "             ('cat', 2),\n",
       "             ('sat', 1),\n",
       "             ('on', 1),\n",
       "             ('mat', 1),\n",
       "             ('where', 1),\n",
       "             ('is', 1)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"the\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which maps the string to an arbitrary integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wchau/MADS-MachineLearning-course-forked/.venv/lib/python3.10/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/wchau/MADS-MachineLearning-course-forked/.venv/lib/python3.10/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "tokenizer.encode(\"the\").ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works for full sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = tokenizer.encode(corpus[0])\n",
    "s1.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An we can map back from the integers to the strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(s1.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that an unknown word is broken down into subwords, or even letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2 = tokenizer.encode(\"barbapapa\")\n",
    "s2.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, token in enumerate(s2.ids):\n",
    "    print(f\"Token #{i} is {token} and {tokenizer.decode([token])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, token in enumerate(s1.ids):\n",
    "    print(f\"Token #{i} is {token} and {tokenizer.decode([token])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see we are missing letters. This is because for BPE, normally you would use a bigger input corpus such that you will encounter at minimum to full vocabulary, and the BPE can always fall back to spelling the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we are now able to map the sentence from strings to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 0, 4]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"First sentence: {corpus[0]}\")\n",
    "tokenized_sentence = tokenizer.encode(corpus[0])\n",
    "tokenized_sentence.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you \"read\" the original sentence?\n",
    "\n",
    "Ok, now, how to represent this. A naive way would be to use a one hot encoding.\n",
    "\n",
    "<img src=https://www.tensorflow.org/text/guide/images/one-hot.png width=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 0],\n",
       "        [1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "tokenized_tensor = torch.tensor(tokenized_sentence.ids)\n",
    "oh = F.one_hot(tokenized_tensor)\n",
    "oh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this might seem like a nice workaround, it is very memory inefficient. \n",
    "Vocabularies can easily grow into the 10.000+ words!\n",
    "\n",
    "So, let's make a more dense space. We simply decide on a dimensionality, and start with assigning a random vector to every word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://www.tensorflow.org/text/guide/images/embedding2.png width=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0231,  0.1896,  0.5405,  1.0020],\n",
       "        [-0.1649, -1.5943,  0.3578, -1.5106],\n",
       "        [-0.4982, -0.4390, -0.7737,  0.0240],\n",
       "        [ 1.3668, -0.0218,  0.4670, -0.5132],\n",
       "        [-1.0231,  0.1896,  0.5405,  1.0020],\n",
       "        [ 0.2561, -0.7609,  0.4631, -1.8276]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"<pad>\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.get_vocab())\n",
    "hidden_dim = 4\n",
    "\n",
    "embedding = torch.nn.Embedding(\n",
    "    num_embeddings=vocab_size, embedding_dim=hidden_dim, padding_idx=0\n",
    ")\n",
    "x = embedding(tokenized_tensor)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So:\n",
    "\n",
    "- we started with a sentence of strings.\n",
    "- we map the strings to arbitrary integers\n",
    "- the integers are used with an Embedding layer; this is nothing more than a lookup table where every word get's a random vector assigned\n",
    "\n",
    "We started with a 6-word sentence. But we ended with a (6, 4) matrix of numbers.\n",
    "\n",
    "So, let's say we have a batch of 32 sentences. We can now store this for example as a (32, 15, 6) matrix: batchsize 32, length of every sentence is 15 (use padding if the sentence is smaller), and every word in the sentence represented with 6 numbers.\n",
    "\n",
    "This is exactly the same as what we did before with timeseries! We have 3 dimensional tensors, (batch x sequence_length x dimensionality) that we can feed into an RNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 16]), torch.Size([1, 6, 16]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ = x[None, ...]\n",
    "rnn = torch.nn.GRU(input_size=hidden_dim, hidden_size=16, num_layers=1)\n",
    "\n",
    "out, hidden = rnn(x_)\n",
    "out.shape, hidden.shape\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a15911aab0965639e9482f052beb89e7ca291bb3f153727c5758e3fe9ad1321e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deep-learning-xB8KIJr7-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
